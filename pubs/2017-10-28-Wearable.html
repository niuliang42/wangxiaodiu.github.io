<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Liang Niu's Blog - A Wearable Assistive Technology for the Visually Impaired with Door Knob Detection and Real-Time Feedback for Hand-to-Handle Manipulation</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Liang Niu</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../me.html">Me</a>
                <a href="../contact.html">Contact</a>
                <a href="../publication.html">Publications</a>
            </div>
        </div>

        <div id="content">
            <h1>A Wearable Assistive Technology for the Visually Impaired with Door Knob Detection and Real-Time Feedback for Hand-to-Handle Manipulation</h1>

            <div class="info">
    Published on October 28, 2017
    
</div>

<p><em>Paper:</em> <a href="https://ieeexplore.ieee.org/document/8265387/">link</a></p>
<p><em>Authors:</em> <strong>Liang Niu</strong>, Cheng Qian, John-Ross Rizzo, Todd Hudson, Zichen Li, Shane Enright, Eliot Sperling, Kyle Conti, Edward Wong, Yi Fang</p>
<p><em>Published on Conference Workshop:</em></p>
<p>ICCV 2017 – 5th International Workshop on Assistive Computer Vision and Robotics (ACVR) – Venice, Italy – October 28th, 2017</p>
<p><em>Abstract:</em></p>
<p>The visually impaired are consistently faced with mobility restrictions due to the lack of truly accessible environments. Even in structured settings, people with low vision may still have trouble navigating efficiently and safely due to hallway and threshold ambiguity. Assistive technologies that are currently available do not provide door and door-handle object detections nor do they concretely help the visually impaired reaching towards the object. In this paper, we propose an AI-driven wearable assistive technology that integrates door handle detection, user’s real-time hand position in relation to this targeted object, and audio feedback for “joy stick-like command” for acquisition of the target and subsequent hand-to-handle manipulation. When fully envisioned, this platform will help end users locate doors and door handles and reach them with feedback, enabling them to travel safely and efficiently when navigating through environments with thresholds. Compared to the usual computer vision models, the one proposed in this paper requires significantly fewer computational resources, which allows it to pair with a stereoscopic camera running on a small graphics processing unit (GPU). This permits us to take advantage of its convenient portability. We also introduce a dataset containing different types of door handles and door knobs with bounding-box annotations, which can be used for training and testing in future research.</p>

        </div>
        <div id="footer">
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
